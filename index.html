
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: left;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 150px;
  font-weight: 600;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}



.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}



</style>


<div class="topnav" id="myTopnav">

</div>


<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>HyperLips: Hyper Control Lips with High Resolution Decoder for Talking Face Generation</title>
    <meta property="og:description" content="HyperLips: Hyper Control Lips with High Resolution Decoder for Talking Face Generation"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>

</head>


 <body>
<div class="container">
    <div class="paper-title">
      <h1>HyperLips: Hyper Control Lips with High Resolution Decoder for Talking Face Generation</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-5 text-center"><a href="https://scholar.google.com.hk/citations?user=HvWZhM4AAAAJ&hl=zh-CN">Yaosen Chen</a><sup>1,2</sup></div>
            <div class="col-5 text-center">Yu Yao</a><sup>2</sup></div>
            <div class="col-5 text-center">Zhiqiang Li</a><sup>2</sup></div>
            <div class="col-5 text-center"><a href="https://scholar.google.com.hk/citations?user=tfJVFEcAAAAJ&hl=zh-CN">Wei Wang</a><sup>2,3</sup></div>
            <div class="col-5 text-center">Yanru Zhang</a><sup>1</sup></div>
            <div class="col-5 text-center">Han Yang</a><sup>1,2</sup></div>
            <div class="col-5 text-center">Xuming Wen</a><sup>2,3</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-8 text-center"><sup>1</sup>University of Electronic Science and Technology of China</a></div>
            <div class="col-8 text-center"><sup>2</sup>Media Intelligence Laboratory, ChengDu Sobey Digital Technology Co., Ltd</a></div>
            <div class="col-8 text-center"><sup>3</sup>Peng Cheng Laboratory</a></div>
        </div>

        <p class="caption">
            *Corresponding Author is Yanru Zhang.
        </p>


        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="https://browse.arxiv.org/pdf/2310.05720.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>

            <a class="supp-btn" href="https://github.com/semchan/HyperLips">
                <span class="material-icons"> description </span> 
                  Code
            </a>
        </div></div>
    </div>


    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="assets/upstnerf.pdf"><img class="screenshot" src="assets/image/paper.png"></a>
            </div>
            <div style="width: 50%">
                <p><b>HyperLips: Hyper Control Lips with High Resolution Decoder for Talking Face Generation</b></p>
                <p>Yaosen Chen, Yu Yao, Zhiqiang Li, Wei Wang, Yanru Zhang, Han Yang, Xuming Wen</p>

                
                <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2310.05720"> arXiv version</a></div>

            </div>
        </div>
    </section>



    <section id="teaser" class="flex-row">
            <a href="assets/image/tear.png" style="text-align: center;">
                <img width="100%" src="assets/image/teaser.png" >
            </a>
        <p class="caption">
            Given the visual face information of source videos (upper left) and driving audio (upper right), our method is capable of rendering and generating more realistic, highfidelity, and lip-synchronized videos (lower). <strong> See the zoom in patches, our method can see details such as teeth.</strong>
        </p>
    </section>
    

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>

        <p>Talking face generation has a wide range of potential applications in the field of virtual digital humans. However, rendering high-fidelity facial video while ensuring lip synchronization is still a challenge for existing audio-driven talking face generation approaches. To address this issue, we propose HyperLips, a two-stage framework consisting of a hypernetwork for controlling lips and a high-resolution decoder for rendering high-fidelity faces. In the first stage, we construct a base face generation network that uses the hypernetwork to control the encoding latent code of the visual face information over audio. First,FaceEncoder is used to obtain latent code by extracting features from the visual face information taken from the video source containing the face frame.Then,HyperConv, which weighting parameters are updated by HyperNet with the audio features as input, will modify the latent code to synchronize the lip movement with the audio.Finally,FaceDecoder will decode the modified and synchr nized latent code into visual face content. In the second stage, we obtain higher quality face videos through a high-resolution decoder. To further improve the quality of face generation, we trained a high-resolution decoder, HRDecoder, using face images and detected sketches generated from the first stage as input. Extensive quantitative and qualitative experiments show that our method outperforms stateof-the-art work with more realistic, high-fidelity, and lip synchronization.
            </p>
    </section>


    <section id="method"/>
        <h2>Approach</h2>
<hr>

    <section id="teaser" class="flex-row">
            <a href="assets/image/overview.png" style="text-align: center;">
                <img width="90%" src="assets/image/overview.png" >
            </a>
        <p class="caption">
            <strong>The overview of our framework is shown above.</strong>We aim to generate a high-fidelity talking face video with synchronized lip movements by implementing the occluded face in the lower half of the input video frame by frame, given an audio and video sequence. Our proposed method consists of two stages: Base Face Generation and High Fidelity Rendering. In Base Face Generation, we designed a hyper-network that takes audio features as input to control the encoding and decoding of visual information to obtain base face images. In high-fidelity rendering, we trained an HRDecoder network using face data from the network trained in the first stage and corresponding face sketches to enhance the base face.
        </p>
    </section>



    </section>









    <section id="results">
        <h2>Visual Comparison</h2>
        <hr>
        <section>
            <div class="flex-row">
                <figure style="width: 100%; text-align: center;">
                    <video width="100%" controls muted loop autoplay>
                        <source src="assets/Visual_Comparison.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
                
            </div>
        </section>
        
        <br/>
        <hr>
    </section>
    
    
    
    
    
    







<hr>

   <section id="Consistency">
        <h2> Quantitative Comparison</h2>

    <section id="teaser" class="flex-row">
            <a href="assets/image/overview.png" style="text-align: center;">
                <img width="100%" src="assets/image/Comparison.png" >
            </a>
        <p class="caption">
            <strong>Table 1 and 2 show the quantitative comparison on the LRS2 and MEAD-Neutral datasets, respectively.</strong>The results show that whether it is our HyperLips-Base or our HyperLips-HR, the generated faces are significantly better than other methods in terms of PSNR, SSIM, and LMD indexes. Our HyperLips-HR is significantly better than our HyperLips-Base in terms of PSNR and SSIM, which shows that our HRDecoder has enhanced high-fidelity face rendering. However, there is no significant increase in the LMD index, which shows that HRDecoder does not help improve lip synchronization. For LSE-C and LSE-D, Wav2Lip perform better results and even outperforms those of ground truth.It only proves that their lip-sync results are nearly comparable to the ground truth, not better.Although LSE-C and LSE-D are not best in our methods, we performed better on the LMD indicator, which is another sync metric metric that measures correspondence in the visual domain.
        </p>
    </section>

    </section>
    




<hr>

   <section id="UserStudy">
        <h2> User study</h2>

    <section id="teaser" class="flex-row">
            <a href="assets/image/overview.png" style="text-align: center;">
                <img width="100%" src="assets/image/user_study.png" >
            </a>
        <p class="caption">
            <strong>User study.</strong>As can be seen, our results stand out from other methods in terms of video quality and lip synchronization.
            
        </p>
    </section>

    </section>





    





</code></pre>
    </section>

<br />
    

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>
        @inproceedings{chen2023HyperLips,
        title = {HyperLips: Hyper Control Lips with High Resolution Decoder for Talking Face Generation},
        author = {Yaosen Chen and Yu Yao and Zhiqiang Li and  Wei Wang and  Yanru Zhang and  Han Yang and Xuming Wen},
        year = {2023},
        booktitle = {arxiv}
        }

</code></pre>
    </section>
</div>
</body>
</html>
